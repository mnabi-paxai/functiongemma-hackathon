
import sys
sys.path.insert(0, "cactus/python/src")
functiongemma_path = "cactus/weights/functiongemma-270m-it"

import json, os, time
from cactus import cactus_init, cactus_complete, cactus_destroy
from google import genai
from google.genai import types


def generate_cactus(messages, tools):
    """Run function calling on-device via FunctionGemma + Cactus."""
    model = cactus_init(functiongemma_path)

    cactus_tools = [{
        "type": "function",
        "function": t,
    } for t in tools]

    raw_str = cactus_complete(
        model,
        [{"role": "system", "content": "You are a helpful assistant that can use tools."}] + messages,
        tools=cactus_tools,
        force_tools=True,
        max_tokens=256,
        stop_sequences=["<|im_end|>", "<end_of_turn>"],
    )

    cactus_destroy(model)

    try:
        raw = json.loads(raw_str)
    except json.JSONDecodeError:
        return {
            "function_calls": [],
            "total_time_ms": 0,
            "confidence": 0,
        }

    return {
        "function_calls": raw.get("function_calls", []),
        "total_time_ms": raw.get("total_time_ms", 0),
        "confidence": raw.get("confidence", 0),
    }


def generate_cloud(messages, tools):
    """Run function calling via Gemini Cloud API."""
    client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

    gemini_tools = [
        types.Tool(function_declarations=[
            types.FunctionDeclaration(
                name=t["name"],
                description=t["description"],
                parameters=types.Schema(
                    type="OBJECT",
                    properties={
                        k: types.Schema(type=v["type"].upper(), description=v.get("description", ""))
                        for k, v in t["parameters"]["properties"].items()
                    },
                    required=t["parameters"].get("required", []),
                ),
            )
            for t in tools
        ])
    ]

    contents = [m["content"] for m in messages if m["role"] == "user"]

    start_time = time.time()

    gemini_response = client.models.generate_content(
        model="gemini-2.5-flash",
        contents=contents,
        config=types.GenerateContentConfig(tools=gemini_tools),
    )

    total_time_ms = (time.time() - start_time) * 1000

    function_calls = []
    for candidate in gemini_response.candidates:
        for part in candidate.content.parts:
            if part.function_call:
                function_calls.append({
                    "name": part.function_call.name,
                    "arguments": dict(part.function_call.args),
                })

    return {
        "function_calls": function_calls,
        "total_time_ms": total_time_ms,
    }


def generate_hybrid(messages, tools, confidence_threshold=0.99):
    """
    Fast-path + self-consistency hybrid inference.

    Strategy:
    1. Fast path: single on-device run. If valid AND high-confidence ->
       return immediately (avoids extra samples for clear, easy cases).
    2. Self-consistency: run N-1 more times, reusing the first run.
       If a majority of valid outputs agree -> return on-device.
    3. If no majority on the full message, decompose compound requests into
       atomic sub-requests and apply fast-path + self-consistency to each.
       - All succeed -> merge and return on-device.
       - Any fails -> single cloud call with the original message.
    4. Cloud fallback.
    """
    import re
    from collections import Counter

    N_SAMPLES = 3

    # ------------------------------------------------------------------ #
    # Helpers                                                              #
    # ------------------------------------------------------------------ #

    def _coerce(value, expected_type):
        if expected_type == "integer":
            if isinstance(value, int) and not isinstance(value, bool):
                return value
            try:
                return int(str(value).strip())
            except (ValueError, TypeError):
                return value
        if expected_type == "number":
            if isinstance(value, (int, float)) and not isinstance(value, bool):
                return value
            try:
                return float(str(value).strip())
            except (ValueError, TypeError):
                return value
        if expected_type == "boolean":
            if isinstance(value, bool):
                return value
            if isinstance(value, str):
                return value.lower() in ("true", "yes", "1")
            return bool(value)
        if expected_type == "string":
            return value if isinstance(value, str) else str(value)
        if expected_type == "array":
            if isinstance(value, list):
                return value
            try:
                parsed = json.loads(value)
                return parsed if isinstance(parsed, list) else [value]
            except Exception:
                return [value]
        return value

    def _validate(calls, tool_list):
        """Validate and type-coerce calls. Returns (corrected, is_valid)."""
        if not calls:
            return calls, False
        tool_map = {t["name"]: t for t in tool_list}
        corrected = []
        for call in calls:
            name = call.get("name")
            if name not in tool_map:
                return calls, False
            schema = tool_map[name].get("parameters", {})
            props = schema.get("properties", {})
            required = schema.get("required", [])
            args = dict(call.get("arguments", {}))
            for field in required:
                if field not in args:
                    return calls, False
            corrected_args = {}
            for k, v in args.items():
                if k in props:
                    coerced = _coerce(v, props[k]["type"])
                    if props[k]["type"] in ("integer", "number") and isinstance(coerced, (int, float)):
                        if coerced < -1000 or coerced > 1_000_000:
                            return calls, False
                    corrected_args[k] = coerced
                else:
                    corrected_args[k] = v
            corrected.append({"name": name, "arguments": corrected_args})
        return corrected, True

    def _fingerprint(calls):
        """Hashable key for a list of function calls (order-independent)."""
        return tuple(sorted(
            (c["name"], tuple(sorted((k, str(v)) for k, v in c["arguments"].items())))
            for c in calls
        ))

    def _self_consistent_with_first(first_run, msgs, tool_list, n=N_SAMPLES):
        """
        Reuse first_run and run n-1 more times. Return majority-consensus
        result or None. Returns (result_or_None, total_time_ms_spent).
        """
        corrected0, is_valid0 = _validate(first_run["function_calls"], tool_list)
        valid_runs = [(corrected0, first_run["confidence"])] if is_valid0 else []
        spent_time = first_run["total_time_ms"]

        for _ in range(n - 1):
            local = generate_cactus(msgs, tool_list)
            spent_time += local["total_time_ms"]
            corrected, is_valid = _validate(local["function_calls"], tool_list)
            if is_valid:
                valid_runs.append((corrected, local["confidence"]))

        if not valid_runs:
            return None, spent_time

        counts = Counter(_fingerprint(r[0]) for r in valid_runs)
        best_key, count = counts.most_common(1)[0]

        if count >= max(2, n // 2 + 1):
            best = max(
                (r for r in valid_runs if _fingerprint(r[0]) == best_key),
                key=lambda r: r[1],
            )
            return {"function_calls": best[0], "confidence": best[1], "total_time_ms": spent_time}, spent_time

        return None, spent_time

    def _on_device(msgs, tool_list):
        """
        Fast-path + self-consistency for a single request.
        Returns (result_or_None, total_time_ms_spent).
        """
        first = generate_cactus(msgs, tool_list)
        corrected, is_valid = _validate(first["function_calls"], tool_list)

        # Fast path: valid + high confidence -> return after 1 sample.
        if is_valid and first["confidence"] >= confidence_threshold:
            return {"function_calls": corrected, "confidence": first["confidence"],
                    "total_time_ms": first["total_time_ms"]}, first["total_time_ms"]

        # Self-consistency: run N-1 more, reuse first.
        return _self_consistent_with_first(first, msgs, tool_list)

    _VERBS = {
        "set", "send", "play", "get", "find", "search", "look", "check",
        "create", "make", "add", "call", "text", "remind", "show", "turn",
        "open", "close", "start", "stop", "enable", "disable", "run",
        "schedule", "book", "cancel", "delete", "update", "read", "wake",
        "fetch", "list", "tell", "ask", "go",
    }

    def _decompose(message):
        parts = re.split(r',\s+and\s+|,\s+', message)
        parts = [re.sub(r'(?i)^and\s+', '', p).strip() for p in parts if p.strip()]
        if len(parts) == 1:
            candidates = [p.strip() for p in re.split(r'\s+and\s+', message, flags=re.IGNORECASE)]
            if (len(candidates) > 1
                    and all(len(p.split()) >= 3 for p in candidates)
                    and all(p.split()[0].lower().rstrip("'s") in _VERBS for p in candidates)):
                parts = candidates
        parts = [p for p in parts if len(p.split()) >= 2]
        return parts if len(parts) > 1 else [message]

    # ------------------------------------------------------------------ #
    # Main routing logic                                                   #
    # ------------------------------------------------------------------ #

    user_msgs = [m for m in messages if m["role"] == "user"]
    non_user  = [m for m in messages if m["role"] != "user"]

    def _resolve(content, depth=0, max_depth=2):
        """
        Recursively resolve a request on-device.
        1. Try fast-path + self-consistency on the content.
        2. If that fails and depth < max_depth, decompose and recurse.
        Returns (calls_list_or_None, time_ms_spent).
        """
        msgs = non_user + [{"role": "user", "content": content}]
        result, time_spent = _on_device(msgs, tools)
        if result is not None:
            return result["function_calls"], time_spent

        if depth < max_depth:
            parts = _decompose(content)
            if len(parts) > 1:
                all_calls = []
                total_time = time_spent
                for part in parts:
                    part_calls, part_time = _resolve(part, depth + 1, max_depth)
                    total_time += part_time
                    if part_calls is None:
                        return None, total_time
                    all_calls.extend(part_calls)
                return all_calls, total_time

        return None, time_spent

    # Step 1 + 2: fast-path then self-consistency on the full message.
    result, spent_time = _on_device(messages, tools)
    if result is not None:
        result["source"] = "on-device"
        return result

    # Step 3: recursive decomposition.
    if user_msgs:
        sub_contents = _decompose(user_msgs[-1]["content"])
        if len(sub_contents) > 1:
            all_calls = []
            total_time = spent_time
            success = True
            for sub_content in sub_contents:
                calls, sub_time = _resolve(sub_content)
                total_time += sub_time
                if calls is None:
                    success = False
                    break
                all_calls.extend(calls)

            if success:
                return {
                    "function_calls": all_calls,
                    "total_time_ms": total_time,
                    "source": "on-device",
                }

            cloud = generate_cloud(messages, tools)
            cloud["source"] = "cloud (fallback)"
            cloud["total_time_ms"] += total_time
            return cloud

    # Step 4: cloud fallback.
    cloud = generate_cloud(messages, tools)
    cloud["source"] = "cloud (fallback)"
    cloud["total_time_ms"] += spent_time
    return cloud


def print_result(label, result):
    """Pretty-print a generation result."""
    print(f"\n=== {label} ===\n")
    if "source" in result:
        print(f"Source: {result['source']}")
    if "confidence" in result:
        print(f"Confidence: {result['confidence']:.4f}")
    if "local_confidence" in result:
        print(f"Local confidence (below threshold): {result['local_confidence']:.4f}")
    print(f"Total time: {result['total_time_ms']:.2f}ms")
    for call in result["function_calls"]:
        print(f"Function: {call['name']}")
        print(f"Arguments: {json.dumps(call['arguments'], indent=2)}")


############## Example usage ##############

if __name__ == "__main__":
    tools = [{
        "name": "get_weather",
        "description": "Get current weather for a location",
        "parameters": {
            "type": "object",
            "properties": {
                "location": {
                    "type": "string",
                    "description": "City name",
                }
            },
            "required": ["location"],
        },
    }]

    messages = [
        {"role": "user", "content": "What is the weather in San Francisco?"}
    ]

    on_device = generate_cactus(messages, tools)
    print_result("FunctionGemma (On-Device Cactus)", on_device)

    cloud = generate_cloud(messages, tools)
    print_result("Gemini (Cloud)", cloud)

    hybrid = generate_hybrid(messages, tools)
    print_result("Hybrid (On-Device + Cloud Fallback)", hybrid)
